Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: WARNING `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id cfnrgo28.
wandb: Tracking run with wandb version 0.18.7
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
/iridisfs/home/oeg1n18/QuantPlaceFinder/tetraenv/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /iridisfs/home/oeg1n18/QuantPlaceFinder/checkpoints/TeTRA-finetune/TeTRA-VitbaseT322_GeM-DescDividerFactor[1] exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loading `train_dataloader` to estimate number of stepping batches.

  | Name       | Type                 | Params | Mode 
------------------------------------------------------------
0 | fp_loss_fn | MultiSimilarityLoss  | 0      | train
1 | fp_miner   | MultiSimilarityMiner | 0      | train
2 | q_loss_fn  | MultiSimilarityLoss  | 0      | train
3 | q_miner    | MultiSimilarityMiner | 0      | train
4 | model      | VPRModel             | 87.6 M | train
------------------------------------------------------------
8.7 M     Trainable params
78.9 M    Non-trainable params
87.6 M    Total params
350.209   Total estimated model params size (MB)
251       Modules in train mode
0         Modules in eval mode
SLURM auto-requeueing enabled. Setting signal handlers.
