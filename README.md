# TeTRAâ€‘VPR: Ternary Transformer for Efficient Visual Place Recognition

Welcome to the official codebase that accompanies the paper **â€œTeTRAâ€‘VPR: A Ternary Transformer Approach for Compact VisualÂ Place Recognitionâ€** (ICRAÂ 2025, preâ€‘printÂ arXiv:2503.02511). This repository provides everything you need to reproduce the twoâ€‘stage training pipelineâ€”**progressive distillation preâ€‘training** and **supervised fineâ€‘tuning**â€”as well as scripts for evaluation and inference on common VPR benchmarks.

---

## Table of Contents

1. [Key Features](#key-features)
2. [Requirements & Installation](#requirements--installation)
3. [Repository Structure](#repository-structure)
4. [Datasets](#datasets)
5. [Configuration System](#configuration-system)
6. [StageÂ 1Â â€“Â Progressive Distillation Preâ€‘Training](#stage-1--progressive-distillation-pre-training)
7. [StageÂ 2Â â€“Â Supervised Fineâ€‘Tuning](#stage-2--supervised-fine-tuning)
8. [Evaluation & Inference](#evaluation--inference)
9. [Preâ€‘Trained Checkpoints](#pre-trained-checkpoints)
10. [Reproducing the Paper](#reproducing-the-paper)
11. [Citation](#citation)
12. [License](#license)
13. [Contact](#contact)

---

## Key Features

* **Ultraâ€‘lowâ€‘bit ViT backbone** â€“ weights quantised to **2â€‘bit ternary** precision; final embeddings **1â€‘bit binary**.
* **Progressive Quantisationâ€‘Aware Training** â€“ smooth sigmoid schedule for stable convergence (Eq.Â 9Â /Â 10 in the paper).
* **Multiâ€‘level Distillation Loss** â€“ aligns classification tokens, patch tokens and attention maps to a DinoV2â€‘BoQ teacher.
* **Plugâ€‘andâ€‘Play Aggregation** â€“ choose between **BoQ**, **SALAD**, **MixVPR** or **GeM** via `--agg_arch`.
* **Turnâ€‘key scripts** â€“ `pretrain.py` and `finetune.py` reproduce all experiments with a single command.

---

## RequirementsÂ &Â Installation

```bash
#Â Create a fresh environment (tested with PythonÂ 3.10)
conda create -n tetra_vpr python=3.10
conda activate tetra_vpr

# Core dependencies
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install pytorch-lightning==2.2.2 timm==0.9.16 faiss-gpu==1.7.4
pip install opencv-python scikit-learn pandas tqdm yacs

# (Optional) Mixedâ€‘precision & BF16 support
pip install xformers==0.0.26
```

> **GPU:** All results were obtained on NVIDIAÂ H100 (80Â GB, SM90). Training also fits on â‰¥24Â GB GPUs (A6000, 4090) with `--accumulate_grad_batches` set appropriately.

---

## RepositoryÂ Structure

```text
â”œâ”€â”€ configs.py              # Dataclass definitions: ModelConfig, DistillConfig, TeTRAConfig
â”œâ”€â”€ pretrain.py             # StageÂ 1 â€“ progressive ternary distillation
â”œâ”€â”€ finetune.py             # StageÂ 2 â€“ supervised binary fineâ€‘tuning
â”œâ”€â”€ data/                   # Data loaders & augmentation pipelines
â”œâ”€â”€ models/                 # ViT backbone, quantisation ops, aggregation heads
â”œâ”€â”€ losses/                 # Distillation + Multiâ€‘Similarity losses
â”œâ”€â”€ scripts/                # Utility scripts (evaluation, export, plotting)
â””â”€â”€ README.md               # You are here ğŸš€
```

---

## Datasets

### 1. **Unlabelled distillation data**

* **SanÂ FranciscoÂ XL panoramas** and **GSVâ€‘Cities/Images**. Place the raw JPEGs under a single root folder, e.g.:

  ```text
  /data/vpr_datasets/gsv-cities/Images/Bangkok/...
  ```

### 2. **Supervised fineâ€‘tuning & validation**

* **GSVâ€‘Cities** (training)   â€“ `--train_dataset_dir`
* **MSLS**, **Pitts30k**, **Tokyo247**, **SVOXâ€‘{Night,Rain,Snow,Sun}** (validation) â€“ group them under `--val_dataset_dir`:

  ```text
  /data/vpr_datasets/MSLS/...
  /data/vpr_datasets/Pitts30k/...
  ...
  ```

> Dataset download links and scripts are provided in `scripts/download_datasets.sh`.

---

## ConfigurationÂ System

All hyperâ€‘parameters are defined in three `@dataclass` objects inside **`configs.py`**:

* **`ModelConfig`** â€“ backbone & aggregation architecture, descriptor normalisation.
* **`DistillConfig`** â€“ learning rate, batch size, augmentation level, etc. for **preâ€‘training**.
* **`TeTRAConfig`** â€“ hyperâ€‘parameters for **fineâ€‘tuning**, freezing policy, quantÂ schedule.

Every field can be overridden from the CLI, e.g. `--batch_size 256`. Run `python pretrain.py --help` for a complete list.

---

## StageÂ 1 â€“ Progressive Distillation Preâ€‘Training

Train the ternary ViT backbone from scratch using unlabeled images.

```bash
python pretrain.py \
  --train_dataset_dir /data/vpr_datasets/gsv-cities/Images \
  --backbone_arch ternaryvitbase \
  --agg_arch boq \
  --lr 4e-4 \
  --batch_size 128 \
  --accumulate_grad_batches 2 \
  --max_epochs 30 \
  --weight_decay 0.01 \
  --use_attn_loss True \
  --precision bf16-mixed
```

Outputs:

* `./checkpoints/pretrain_epoch=29.ckpt` â€“ lightning checkpoint (â‰ˆÂ 47Â MB).
* `./logs/` â€“ TensorBoard event files with loss curves & quantâ€¯Î»(t).

âš ï¸Â **Tips**

* Use `--noramlize False` if you plan to integrate with aggregation heads that already normalise outputs.
* Mixed precision speeds up training by \~30â€¯% on Ampere and newer GPUs.

---

## StageÂ 2 â€“ Supervised Fineâ€‘Tuning

Fineâ€‘tune aggregation head + final ViT block using place labels.

```bash
python finetune.py \
  --pretrain_checkpoint ./checkpoints/pretrain_epoch=29.ckpt \
  --train_dataset_dir /data/vpr_datasets/gsv-cities \
  --val_dataset_dir   /data/vpr_datasets \
  --cities Bangkok London Rome "LosAngeles" \
  --agg_arch boq \
  --quant_schedule logistic \
  --freeze_backbone True \
  --freeze_all_except_last_n 1 \
  --lr 1e-4 \
  --batch_size 200 \
  --max_epochs 40 \
  --precision bf16-mixed
```

Outputs:

* `./checkpoints/finetune_boQ_epoch=39.ckpt` â€“ final binaryâ€‘embedding model (â‰ˆÂ 49Â MB).
* `./embeddings/*.bin` â€“ optional cached descriptors for validation sets.

---

## EvaluationÂ &Â Inference

Generate binary descriptors & compute Recall\@1 on **Tokyo247**:

```bash
python scripts/eval.py \
  --checkpoint ./checkpoints/finetune_boQ_epoch=39.ckpt \
  --dataset Tokyo247 --topk 100
```

*Uses FAISS Hamming index; expects query/reference split under the dataset folder.*

For **realâ€‘time localisation** on a video stream:

```bash
python scripts/stream_localisation.py --checkpoint <ckpt> --source 0
```

---

## Preâ€‘Trained Checkpoints

| Model        | Aggregation | Descriptor Dim | R\@1Â (Tokyo247) | Size  | Link                                              |
| ------------ | ----------- | -------------- | --------------- | ----- | ------------------------------------------------- |
| TeTRAâ€‘BoQ    | BoQ         | 1024 (binary)  | **88.6Â %**      | 49Â MB | [download](https://example.com/tetra_boQ.ckpt)    |
| TeTRAâ€‘SALAD  | SALAD       | 256 (binary)   | 85.4Â %          | 45Â MB | [download](https://example.com/tetra_salad.ckpt)  |
| TeTRAâ€‘MixVPR | MixVPR      | 512 (binary)   | 82.5Â %          | 46Â MB | [download](https://example.com/tetra_mixvpr.ckpt) |

---

## Reproducing the Paper

To replicate TablesÂ IÂ &Â III from the manuscript:

```bash
bash scripts/reproduce_icra25.sh  # runs all benchmarks & logs metrics to CSV
python scripts/plot_tradeoffs.py   # recreates Fig.Â 3 &Â 4
```

> **Expected hardware:** 1Ã—Â A100Â 80GB or 4Ã—Â RTXÂ 4090 (gradient accumulation=3).

---

## Citation

If you use this codebase or the pretrained models, please cite:

```bibtex
@article{Grainge2025TeTRA,
  title   = {TeTRA--VPR: A Ternary Transformer Approach for Compact Visual Place Recognition},
  author  = {Oliver Grainge and Michael Milford and Indu Bodala and Sarvapali~D.~Ramchurn and Shoaib Ehsan},
  journal = {IEEE Transactions on Robotics},
  year    = {2025},
  note    = {arXiv:2503.02511}
}
```

---

## License

This project is released under the **MIT License**. SeeÂ [`LICENSE`](LICENSE) for details.

---

## Contact

* **OllieÂ Grainge** â€“ [oeg1n18@soton.ac.uk](mailto:oeg1n18@soton.ac.uk)
* Pull requests are welcome! Open an issue for feature requests or bug reports.

> *â€œA place is worth a bag of ternary queries.â€*  â€“ TeTRA motto ğŸ›°ï¸ğŸš€
